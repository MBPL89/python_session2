{
  "hash": "27ae5e88050c0f16417d80c06afa1c0b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Week 10 Starter File\"\nauthor: \"Biagio Palese\"\nengine: knitr\nformat: live-html\nwebr:\n  packages:\n    - tidyverse\n    - tidymodels\n    - corrplot\n    - janitor\n    - glmnet \neditor_options: \n  chunk_output_type: console\n---\n\n```{=html}\n<style>\n/* Target the first tab (Demo) */\n.nav-pills .nav-item:nth-child(1) .nav-link {\n  background-color: #FFC20A; /* Yellow */\n  color: white;\n}\n\n/* Target the second tab (Example) */\n.nav-pills .nav-item:nth-child(2) .nav-link {\n  background-color: #5D3A9B; /* Purple */\n  color: white;\n}\n\n/* Target the third tab (Exercise) */\n.nav-pills .nav-item:nth-child(3) .nav-link {\n  background-color: #0C7BDC; /* Blue */\n  color: white;\n}\n\n/* Target the fourth tab (Activity) */\n.nav-pills .nav-item:nth-child(4) .nav-link {\n  background-color: #E66100; /* Orange */\n  color: white;\n}\n\n/* Target the fifth tab (Activity2) */\n.nav-pills .nav-item:nth-child(5) .nav-link {\n  background-color: #40B0A6; /* teal */\n  color: white;\n}\n\n/* Active tab styling */\n.nav-pills .nav-link.active {\n  background-color: #2E7D32; /* Customize active tab color */\n  color: white;\n}\n</style>\n```\n::: {.cell}\n\n:::\n\n\n\n# Intermediate Modeling\n\n![Coding is fun!!!](gif/coding_is_fun.gif){width=\"700\"}\n\nIn the second part of the course we will leverage the following resources:\n\n-   The Tidy Modeling with R book [second portion of the course book](https://www.tmwr.org)\n-   The Tidy Models website [second portion of the course website](https://www.tidymodels.org)\n\n#### Link to other resources\n\n-   Internal help: [posit support](https://support.posit.co/hc/en-us){style=\"text-decoration: underline; color: rgb(0, 0, 255) !important;\"}\n\n-   External help: [stackoverflow](https://stackoverflow.com/search?q=rstudio&s=25d8522e-3191-4bf2-ae3b-ccad762aeca9){style=\"text-decoration: underline; color: rgb(0, 0, 255) !important;\"}\n\n-   Additional materials: [posit resources](https://posit.co/resources/){style=\"text-decoration: underline; color: rgb(0, 0, 255) !important;\"}\n\nWhile I use the book as a reference the materials provided to you are custom made and include more activities and resources.\n\nIf you understand the materials covered in this document there is no need to refer to other resources.\n\nIf you have any troubles with the materials don't hesitate to contact me or check the above resources.\n\n![Data Science model: Artwork by @allison_horst](images/environmental-data-science-r4ds-general.png)\n\n::: {#custom-timer}\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"countdown\" id=\"timer_aacb7a5a\" data-warn-when=\"180\" data-update-every=\"1\" data-play-sound=\"true\" data-blink-colon=\"true\" tabindex=\"0\" style=\"right:0;bottom:0;font-size:1.5rem;\">\n<div class=\"countdown-controls\"><button class=\"countdown-bump-down\">&minus;</button><button class=\"countdown-bump-up\">&plus;</button></div>\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">05</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n\n:::\n:::\n\n:::\n\n\n```{=html}\n<style>\n  /* Fix the custom timer div to the top-right corner */\n  #custom-timer {\n    position: fixed;    /* Ensure the timer stays fixed when scrolling */\n    top: 70px;          /* Distance from the top */\n    right: 10px;        /* Distance from the right */\n    font-size: 1rem;   /* Small font size */\n    z-index: 9999;       /* Ensure it stays on top of other elements */\n    width: 250px;        /* Set a small width for the timer */\n    text-align: center;  /* Center the text */\n  }\n</style>\n```\n\n### Class Objectives\n\n-   Going beyond the basic by making predictions and assessing multiple models.\n\n## Load packages\n\nThis is a critical task:\n\n-   Every time you open a new R session you will need to load the packages.\n\n-   Failing to do so will incur in the most common errors among beginners (e.g., \" could not find function 'x' \" or \"object 'y' not found\").\n\n-   So please always remember to load your packages by running the `library` function for each package you will use in that specific session ü§ù\n\n\n::: {.cell autorun='true' startover='false'}\n```{webr}\n#| autorun: true\n#| startover: false\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(corrplot)\nlibrary(janitor)\n```\n:::\n\n\n## The tidymodels Ecosystem in action\n\n![Tidymodels in Action](images/tidymodels_ecosystem_inpractice.png)\n\n\n## Some additional useful/required steps\n\nSo far we have covered the basic of modeling and we have learned how to create recipes, define models and embed them into workflows. However, from experience there is a set of other things that are important and need to be taken care in addition of what we learned in the model basic part.\n\n### Data cleaning and generalizable manipulations\n\n![Time for some permanent cleaning!](images/janitor.png)\n\nIn the previous weeks we have made some repetitive transformations (e.g., log Sale_Price) and encountered constant issues with our dataset (e.g., upper and lower case columns names). It is now time to clean the data and make model independent useful manipulations before moving back to modeling\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\names_clean <- ames |> \n  clean_names() |> \n  #clean column name and put everything lowercase thanks to the janitor package\n  mutate(price_log= log(sale_price ,base = 10), \n         total_sf= total_bsmt_sf+first_flr_sf+second_flr_sf,\n         total_bath= bsmt_full_bath+bsmt_half_bath+full_bath+half_bath) |> \n    # apply manipulations that are needed across multiple models like transforming sale_price and feature engineering to create two important columns\n  select(-c(sale_price, total_bsmt_sf, first_flr_sf, second_flr_sf,\n            bsmt_full_bath, bsmt_half_bath, full_bath, half_bath))# remove redundant columns\n\nglimpse(ames_clean)\n\n```\n:::\n\n\n::: callout-caution\nI have removed the variables that are highly correlated with the variables we will use in the models below. Remember with mutate we have created those variables from existing variables in our dataset. If you don't remove them they will bias your results due to multicollinearity (high correlation with the original ones).\n:::\n\n\n### Check variables correlations\n\nCheck variables correlation second.\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\ncorrelation_matrix <- ames_clean |>\n  select(4, 18, 40, 47, 67:69 ) |>\n  cor(use = \"complete.obs\")# exploring correlation is a great step to identify those variables that seem to have a relation between each other and with your dependent variable.  \n#?cor\ncorrelation_matrix |> \n  corrplot()# visualizing the matrix make it is easier and more accessible to check correlation between variables. It is easy to create this visual thanks to the corrplot package\n```\n:::\n\n\nA positive (blue) or negative (red) correlation between two variables indicates the direction of the relationship between them:\n\n-   **Positive Correlation:** A positive correlation indicates that the two variables move in the same direction. When one variable increases, the other variable also tends to increase, and similarly, when one decreases, the other tends to decrease. This relationship reflects a direct association between the variables, where changes in one variable are mirrored by changes in the other in the same direction, either upwards or downwards. The correlation is considered positive whenever its value is greater than 0. A perfect positive correlation, with a coefficient of +1, means that the two variables move in the same direction in a perfectly linear manner.\n\n-   **Negative Correlation:** A negative correlation indicates that the two variables move in the opposite directions. When one variable increases, the other variable tends to decrease, and viceversa. This inverse relationship means that changes in one variable are associated with opposite changes in the other, highlighting an indirect association between them. The correlation is considered negative whenever its value is smaller than 0. A perfect negative correlation, with a coefficient of -1, means that the two variables move in opposite directions in a perfectly linear manner.\n\n**Interpreting Correlations:**\n\n-   **Magnitude of the Coefficient:** The closer the correlation coefficient is to +1 or -1, the stronger the linear relationship between the variables. A correlation of 0 indicates no linear relationship.\n\n-   **Significance of the Correlation:** It's also important to assess the statistical significance of the correlation. A correlation coefficient might indicate a positive or negative relationship, but statistical tests (like a significance test for correlation) can determine if the observed correlation is not likely due to random chance. These tests are beyond the scope of the class but you need to be aware that it is good practice to perform them.\n\n-   **Causation:** It's crucial to remember that correlation does not imply causation. Even if two variables have a strong positive or negative correlation, it does not mean that one variable causes the changes in the other. Other variables, known as confounding variables, could influence the relationship, or it could be coincidental. Regression will help us in understanding how the dependent variable changes with an independent variable, holding other factors constant. This is particularly useful for controlling for potential confounders and examining the specific impact of one variable on another.\n\n-   **Chart interpretation:** By looking at our correlation plot we can tell that all variables selected have a positive correlation with `price_log`. It appears that the stronger correlation is with `total_sf` which means that larger houses tend to sell for higher prices (when one increases the other does the same). The only variables that are slightly negatively correlated are `bedroom_abv` and `year_built`, which is interesting. Try to interpret more variables in the chart on your own.\n\n#### Activity 1: Computing, visualizing and interpreting correlation. Write the code to complete the tasks below - 8 minutes\n\n*\\[Write code just below each instruction; finally use MS Teams R - Forum channel for help on the in class activities/homework or if you have other questions\\]*\n\n::: {.panel-tabset .nav-pills}\n## Activity 1a.\n\nCreate a \"correlation_matrix2\" that includes `price_log`, `kitchen_abv_gr`, `tot_rms_abv_grd`, `fireplaces`, `garage_cars`, `garage_area`, `wood_deck_sf`, `open_porch_sf`, `longitude`, `latitude`.\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\n\n```\n:::\n\n\n## Activity 1b.\n\nCreate a plot that visually shows the correlation between the variables computed with the above matrix.\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\n\n```\n:::\n\n\n## Activity 1c.\n\nInterpret the correlation chart. Here are some prompts for you: What variable/s have positive correlation with sale_price? Pick one and interpret what it means. What about variables with negative correlation? Pick one and interpret what it means. Anything surprising?\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\n\n#Write your interpretation here\n```\n:::\n\n:::\n\n\n::: {.cell autorun='false' min-lines='3'}\n```{webr}\n#| autorun: false\n#| min-lines: 3\n\n\n#Code Practice Chunk\n\n```\n:::\n\n\n### Data Splitting and Cross-validation\n\nNow that we have checked the correlation and made some changes to our ames dataset, we can move back to modeling. So far we have run our models on the entire ames dataset. However, by doing so we have no data left to evaluate and assess our model performance on unseen data. Here are the two commonly used methods to preserve data for assessing models performance:\n\n-   Data Splitting\n\n-   Cross-validation\n\nWhile a simple train-test split is quicker and easier, especially for exploratory analysis or when computational resources are limited, cross-validation provides a more thorough and unbiased evaluation of the model's performance. The choice between these methods depends on the specific needs of your project, including the size of the dataset, the computational complexity of the models, and the level of accuracy required in the performance estimation. Due to time constrains we will only learn train-test split but remember that cross-validation might be what is required to assess models in some of your future data analytics projects.\n\n### Data Splitting\n\nWhen it comes to data modeling (and model evaluation), one of the most adopted method is to split the data into a training set and a test set from the beginning. Here‚Äôs how this simple method can be implemented and its implications:\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\n# Split the data\nset.seed(123)# setting a seed will ensure reproducibility. The number can be any random number as long as you always use the same\names_split <- initial_split(ames_clean, prop = 0.75) # define the split and its data proportion\names_train <- training(ames_split) #create a train set\names_test <- testing(ames_split)#create a test set\n```\n:::\n\n\n-   **Benefits of Train-Test Split:**\n    -   **Simplicity and Speed:** It's straightforward to understand, implement and computationally less intensive than other methods.\n    -   **Direct Evaluation:** Provides a clear, direct way to assess how the model performs on unseen data.\n-   **Something to consider:**\n    -   **Potential Data Wastage:** Splitting the dataset reduces the amount of data available for training the model, which might be a concern for smaller datasets.\n    -   **Risk of Bias:** If the split is not representative, it can introduce bias in the evaluation, making the model appear to perform better or worse than it actually does. Less robust than other methods.\n\n#### Activity 2: Data Splitting. - 5 minutes\n\n*\\[Write code just below each instruction; finally use MS Teams R - Forum channel for help on the in class activities/homework or if you have other questions\\]*\n\n::: {.panel-tabset .nav-pills}\n## Activity 2a.\n\nSet a random seed to reproduce the data splitting.\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\n\n```\n:::\n\n\n## Activity 2b.\n\nDefine a data split with a 80/20 proportion, call the split as \"ames_split2\". Compare the size of the ames_split2 with the one of the ames_split.\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\n\n```\n:::\n\n\n## Activity 2c.\n\nCreate a ames_train2 and a ames_test2 object.\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\n\n#Write your interpretation here\n```\n:::\n\n:::\n\n\n::: {.cell autorun='false' min-lines='3'}\n```{webr}\n#| autorun: false\n#| min-lines: 3\n\n\n#Code Practice Chunk\n\n```\n:::\n\n\n### Example: Making prediction and evaluating different models' prediction performance with data splitting\n\nIt is time to use a practical example to see how we can define different models and how we can assess them. We will explore the predictions' outcome among linear regression, lasso regression and decision tree. We will do that using different recipes customized to the restrictions/needs of the different models. We will use recipe 1 and 2 with the multiple linear regression models and recipe 3 and 4 with our decision tree model.\n\n### Setting up the example recipes\n\n::: {.panel-tabset .nav-pills}\n## Recipe 1\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\nrecipe1 <- recipe(price_log ~ total_sf + total_bath, data = ames_train) |>\n  step_normalize(all_numeric_predictors())# regression model with 2 variables\n\nrecipe1# Review the recipe steps\n```\n:::\n\n\n## Recipe 2\n\n\n::: {.cell}\n```{webr}\nrecipe2 <- recipe(price_log ~ ., data = ames_train) |>\n  step_normalize(all_numeric_predictors()) |> \n  step_dummy(all_nominal_predictors())\nrecipe2# Review the recipe steps\n\n```\n:::\n\n\n## Recipe 3\n\n\n::: {.cell}\n```{webr}\nrecipe3 <- recipe(price_log ~ total_sf + total_bath + overall_cond,  data = ames_train)# regression model with 3 variables\n\nrecipe3# Review the recipe steps\n```\n:::\n\n\n## Recipe 4\n\n\n::: {.cell}\n```{webr}\nrecipe4 <- recipe(price_log ~ ., data = ames_train)\n\nrecipe4# Review the recipe steps\n```\n:::\n\n:::\n\n\n:::callout-important\nIn real life you should prep and bake recipe 1 and 2 to verify that the preprocessing steps applied lead to the desired outcomes.\n:::\n\n\n### Specifying the models\n\nWe will specify two regression models and a decision tree-based model using `parsnip`. Please see below a brief overview of the models we will use to create and evaluate predictions:\n\n-   **Linear Regression**\n\n[When to Use]{.underline}: Ideal for predicting continuous outcomes when the relationship between independent variables and the dependent variable is linear.\n\n[Example]{.underline}: Predicting house prices based on attributes like size (square footage), number of bedrooms, and age of the house.\n\n-   **Lasso Regression**\n\n[When to Use]{.underline}: Similar to Ridge regression but can shrink some coefficients to zero, effectively performing variable selection (L1 regularization). Useful for models with a large number of predictors, where you want to identify a simpler model with fewer predictors.\n\n[Example]{.underline}: Selecting the most impactful factors affecting the energy efficiency of buildings from a large set of potential features.\n\n-   **Decision Tree**\n\n[When to Use]{.underline}: Good for classification and regression with a dataset that includes non-linear relationships. Decision trees are interpretable and can handle both numerical and categorical data.\n\n[Example]{.underline}: Predicting customer churn based on a variety of customer attributes such as usage patterns, service complaints, and demographic information.\n\n::: {.panel-tabset .nav-pills}\n## Linear reg\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\n# Linear regression with lm\nlinear_mod_reg <- linear_reg() |>\n  set_engine(\"lm\") |>\n  set_mode(\"regression\")\n\n```\n:::\n\n\n## Lasso reg\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\n# Lasso regularized linear regression with glmnet\nlinear_mod_lasso <- linear_reg(penalty = 0.1, mixture = 1) |>\n  set_engine(\"glmnet\") |>\n  set_mode(\"regression\")\n```\n:::\n\n\n## Decision tree\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\ndecision_tree_mod <- decision_tree() |>\n  set_engine(\"rpart\") |>\n  set_mode(\"regression\")\n```\n:::\n\n:::\n\n### Fitting the models using workflows\n\nNext, we fit the models to the `ames_train` dataset because we want to assess their predictions performance on the `ames_test`. This is accomplished by embedding the model specifications within `workflows` that also incorporate our preprocessing `recipes`.\n\n### Regression models workflows\n\n::: {.panel-tabset .nav-pills}\n## Linear reg recipe 1\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\nrecipe1_workflow_reg <- workflow() |>\n  add_recipe(recipe1) |>\n  add_model(linear_mod_reg) |>\n  fit(data = ames_train)#we create and fit the workflow on just the train set\nrecipe1_workflow_reg |>tidy()#check model results\n\n```\n:::\n\n\n## Linear reg recipe 2\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\nrecipe2_workflow_reg <- workflow() |>\n  add_recipe(recipe2) |>\n  add_model(linear_mod_reg) |>\n  fit(data = ames_train)#we create and fit the workflow on just the train set\nrecipe2_workflow_reg |> tidy() |>print(n=272)#check model results\n\n```\n:::\n\n\n## Lasso reg recipe 1\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\nrecipe1_workflow_lasso <- workflow() |>\n  add_recipe(recipe1) |>\n  add_model(linear_mod_lasso) |>\n  fit(data = ames_train)#we create and fit the workflow on just the train set\nrecipe1_workflow_lasso |> tidy()#check model results\n```\n:::\n\n\n## Lasso reg recipe 2\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\nrecipe2_workflow_lasso <- workflow() |>\n  add_recipe(recipe2) |>\n  add_model(linear_mod_lasso) |>\n  fit(data = ames_train)#we create and fit the workflow on just the train set\nrecipe2_workflow_lasso |> tidy() |>print(n=272)#check model results\n```\n:::\n\n:::\n\n### Tree-based models workflows\n\n::: {.panel-tabset .nav-pills}\n## Decision tree recipe 3\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\nrecipe3_workflow_dt <- workflow() |>\n  add_recipe(recipe3)|>\n  add_model(decision_tree_mod) |>\n  fit(data = ames_train)#we create and fit the workflow on just the train set\nrecipe3_workflow_dt #check model results\n```\n:::\n\n\n## Decision tree recipe 4\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\nrecipe4_workflow_dt <- workflow() |>\n  add_recipe(recipe4)|>\n  add_model(decision_tree_mod) |>\n  fit(data = ames_train)#we create and fit the workflow on just the train set\nrecipe4_workflow_dt#check model results\n\n```\n:::\n\n:::\n\n::: callout-important\nDon't worry we will learn how to interpret and visualize these models in the next class.\n:::\n\n# Model Comparison and Evaluation\n\nBefore learning how to interpret all the models results, we will learn to assess their performance (there is no point in interpreting them if they perform poorly or are \"bad\" models). This involves making predictions on our test set, and evaluating the models using metrics suited for regression tasks, such as `RMSE (Root Mean Squared Error)` or `MAE (Mean Absolute Error)` or `R¬≤ (Coefficient of Determination)`.\n\n## Making predictions and Evaluating Model Performance with Data Splitting\n\n### Regression models predictions\n\n::: {.panel-tabset .nav-pills}\n## Linear reg recipe 1 predictions\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\n# Making predictions for the linear regression models workflows\nrecipe1_predictions_reg <- predict(recipe1_workflow_reg, new_data = ames_test) |> # making the prediction on test data (unseen data)\n  bind_cols(ames_test) |> # add original columns\n   mutate(sale_price= 10^price_log, pred = 10^.pred)  # Inverse of log10, easier to interpret the prediction now that are not in a log10 form\n\nrecipe1_predictions_reg |> \n  select(sale_price, pred)#check the prediction on test data against actual value (truth)\n\n```\n:::\n\n\n::: callout-important\nCan we compute the difference between the sale_price and the predicted price? What about the sum of the errors? \\[hint: use mutate and summarize\\]\n:::\n\n## Linear reg recipe 2 predictions\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\nrecipe2_predictions_reg <- predict(recipe2_workflow_reg, new_data = ames_test) |> \n  bind_cols(ames_test) |> # add original columns\n   mutate(sale_price= 10^price_log, pred = 10^.pred)   # Inverse of log10, easier to interpret the prediction now that are not in a log10 form\n\n\nrecipe2_predictions_reg |> \n  select(sale_price, pred)#check the prediction on test data against actual value (truth)\n```\n:::\n\n\n## Lasso reg recipe 1 predictions\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\n# Making predictions for the lasso models workflows\nrecipe1_predictions_lasso <- predict(recipe1_workflow_lasso, new_data = ames_test) |> \n  bind_cols(ames_test) |> # add original columns\n   mutate(sale_price= 10^price_log, pred = 10^.pred)  # Inverse of log10, easier to interpret the prediction now that are not in a log10 form\n\nrecipe1_predictions_lasso |> \n  select(sale_price, pred) #check the prediction on test data against actual value (truth)\n```\n:::\n\n\n## Lasso reg recipe 2 predictions\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\nrecipe2_predictions_lasso <- predict(recipe2_workflow_lasso, new_data = ames_test) |> \n  bind_cols(ames_test) |> # add original columns\n   mutate(sale_price= 10^price_log, pred = 10^.pred)   # Inverse of log10, easier to interpret the prediction now that are not in a log10 form\n\nrecipe2_predictions_lasso |> \n  select(sale_price, pred)#check the prediction on test data against actual value (truth)\n\n```\n:::\n\n:::\n\n### Tree-based models predictions\n\n::: {.panel-tabset .nav-pills}\n\n## Decision tree recipe 3 predictions\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\n# Making predictions for the decision tree models workflows\nrecipe3_predictions_dt <- predict(recipe3_workflow_dt, new_data = ames_test) |> \n  bind_cols(ames_test) |> # add original columns\n   mutate(sale_price= 10^price_log, pred = 10^.pred)   # Inverse of log10, easier to interpret the prediction now that are not in a log10 form\n\nrecipe3_predictions_dt |> \n  select(sale_price, pred) #check the prediction on test data against actual value (truth)\n```\n:::\n\n\n## Decision tree recipe 4 predictions\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\nrecipe4_predictions_dt <- predict(recipe4_workflow_dt, new_data = ames_test) |> \n  bind_cols(ames_test) |> # add original columns\n   mutate(sale_price= 10^price_log, pred = 10^.pred)# Inverse of log10, easier to interpret the prediction now that are not in a log10 form\n\nrecipe4_predictions_dt |> \n  select(sale_price, pred)#check the prediction on test data against actual value (truth)\n\n```\n:::\n\n\n:::\n\n::: callout-caution\nIf you guys noticed when we run predictions using the linear regression model with recipe2 (recipe2_predictions_reg) we got a warning. The warning indicated some rank-deficiency fit. This is usually due to:\n\n-   the presence of highly correlated predictors (multicollinearity). Multicollinearity indicates that we have redundant information from some highly correlated predictors, making it difficult to distinguish their individual effects on the dependent variable.\n\n-   the presence of too many predictors in our model. If the model includes too many predictor variables relative to the number of observations, it can lead to a situation where the predictors cannot be uniquely identified. Meaning that there isn't enough independent information in the data to estimate the model's parameters (the coefficients of the predictor variables) with precision.\n\nPossible solutions are check for multicollinearity and using correlation matrix to identify and then remove highly correlated predictors. Or reduce the number of predictors by performing Principal Components Analysis (PCA). PCA is beyond the scope of this class but regularization methods (e.g., Ridge or Lasso regression) are designed to handle multicollinearity (Ridge in particular), high number of predictors (Lasso in particular) and overfitting. For this reason we don't get a warning when we run the lasso model using recipe2.\n\nIn conclusion, while the linear regression model with recipe2 runs (and produce just a warning), we should not attempt to interpret the results because they can misleading and lead to bad decisions. For illustrative scope we will keep that model in but we know that in real life we will have to make changes to what predictors are included in it.\n:::\n\n## Creating Model Metrics to Assess Model Prediction Performance\n\nWhile seeing the predictions next to the actual house values can already provide some insights on the goodness of the model. In regression analysis, model performance is evaluated using specific metrics that quantify the model's accuracy and ability to generalize. Three fundamental metrics are Root Mean Squared Error (RMSE) , Mean Absolute Error (MAE), and R-squared (R¬≤):\n\n-   **Root Mean Squared Error (RMSE):**\n    -   **What It Measures:** RMSE calculates the square root of the average squared differences between the predicted and actual values. It represents the standard deviation of the residuals (prediction errors).\n    -   **Interpretation:** A lower RMSE value indicates better model performance, with 0 being the ideal score. It quantifies how much, on average, the model's predictions deviate from the actual values.\n    -   **Something to consider:** RMSE is sensitive to outliers. High RMSE values may suggest the presence of large errors in some predictions, highlighting potential model weaknesses.\n-   **Mean Absolute Error (MAE)**:\n    -   **What It Measures**: MAE quantifies the average magnitude of the errors between the predicted values and the actual values, focusing solely on the size of errors without considering their direction. It reflects the average distance between predicted and actual values across all predictions.\n    -   **Interpretation**: MAE values range from 0 to infinity, with lower values indicating better model performance. A MAE of 0 means the model perfectly predicts the target variable, although such a scenario is extremely rare in practice.\n    -   **Something to consider**: MAE provides a straightforward and easily interpretable measure of model prediction accuracy. It's particularly useful because it's robust to outliers, making it a reliable metric when dealing with real-world data that may contain anomalies. MAE helps in understanding the typical error magnitude the model might have in its predictions, offering clear insights into the model‚Äôs performance.\n-   **R-squared (R¬≤):**\n    -   **What It Measures:** R¬≤, also known as the coefficient of determination, quantifies the proportion of the variance in the dependent variable that is predictable from the independent variables. It provides a measure of how well observed outcomes are replicated by the model.\n    -   **Interpretation:** R¬≤ values range from 0 to 1, where higher values indicate better model fit. An R¬≤ of 1 suggests the model perfectly predicts the target variable.\n    -   **Something to consider:** R¬≤ offers an insight into the goodness of fit of the model. However, it does not indicate if the model is the appropriate one for your data, nor does it reflect on the accuracy of the predictions.\n\n### Regression models performance metrics\n\n::: {.panel-tabset .nav-pills}\n## Recipe 1 Linear regression metrics\n\n\n::: {.cell}\n```{webr}\nrecipe1_lm_reg_model <- recipe1_predictions_reg |> \n   metrics(truth = sale_price, estimate = pred) |> \n   mutate(model= \"recipe1_lm_reg_model\") # Adding recipe1_reg_model to distinguish with the others models. Remember: rmse the smaller the better; mae the smaller the better; rsq the bigger the better\nrecipe1_lm_reg_model\n```\n:::\n\n\n## Recipe 2 Linear regression metrics\n\n\n::: {.cell}\n```{webr}\nrecipe2_lm_reg_model <- recipe2_predictions_reg |> \n   metrics(truth = sale_price, estimate = pred) |> \n   mutate(model= \"recipe2_lm_reg_model\") # Adding recipe2_lm_reg_model to distinguish with the others models. Remember: rmse the smaller the better; mae the smaller the better; rsq the bigger the better\nrecipe2_lm_reg_model\n```\n:::\n\n\n## Recipe 1 Lasso regression metrics\n\n\n::: {.cell}\n```{webr}\nrecipe1_lasso_reg_model <- recipe1_predictions_lasso |> \n   metrics(truth = sale_price, estimate = pred) |> \n   mutate(model= \"recipe1_lasso_reg_model\") # Adding recipe1_lasso_reg_model to distinguish with the others models. Remember: rmse the smaller the better; mae the smaller the better; rsq the bigger the better\nrecipe1_lasso_reg_model\n```\n:::\n\n\n## Recipe 2 Lasso regression metrics\n\n\n::: {.cell}\n```{webr}\nrecipe2_lasso_reg_model <- recipe2_predictions_lasso |> \n   metrics(truth = sale_price, estimate = pred) |> \n   mutate(model= \"recipe2_lasso_reg_model\") # Adding recipe2_lasso_reg_model to distinguish with the others models. Remember: rmse the smaller the better; mae the smaller the better; rsq the bigger the better\nrecipe2_lasso_reg_model\n```\n:::\n\n:::\n\n### Decision tree models performance metrics\n\n::: {.panel-tabset .nav-pills}\n## Recipe 3 Decision tree metrics\n\n\n::: {.cell}\n```{webr}\nrecipe3_decision_tree_model <- recipe3_predictions_dt |> \n   metrics(truth = sale_price, estimate = pred) |> \n   mutate(model= \"recipe3_decision_tree_model\") # Adding recipe3_decision_tree_model to distinguish with the others models. Remember: rmse the smaller the better; mae the smaller the better; rsq the bigger the better\nrecipe3_decision_tree_model\n```\n:::\n\n\n## Recipe 4 Decision tree metrics\n\n\n::: {.cell}\n```{webr}\nrecipe4_decision_tree_model <- recipe4_predictions_dt |> \n   metrics(truth = sale_price, estimate = pred) |> \n   mutate(model= \"recipe4_decision_tree_model\") # Adding recipe4_decision_tree_model to distinguish with the others models. Remember: rmse the smaller the better; mae the smaller the better; rsq the bigger the better\nrecipe4_decision_tree_model\n```\n:::\n\n:::\n\n### Identify the best model using models' metrics\n\nDecide which model to proceed with should be based on these metrics, considering `RMSE \\[lower values better\\]`, `MAE \\[lower values better\\]` and `R¬≤ \\[higher values better\\]`. Sometimes the best model is the one that gives the best compromise among those metrics. They are not always in agreement. Moreover, keep in mind that the choice of model might also depend on other factors such as:\n\n-   Interpretability and complexity of the model.\n\n-   Computational resources and time available.\n\n-   The specific requirements of your application or project.\n\nLet's check all the models metrics to establish the best model keeping in mind the above bullet points.\n\n## Combine All Metrics and Compare\n\n\n::: {.cell}\n```{webr}\nall_models_metrics <- bind_rows(\n  recipe1_lm_reg_model, recipe2_lm_reg_model, recipe1_lasso_reg_model, recipe2_lasso_reg_model,\n  recipe3_decision_tree_model, recipe4_decision_tree_model\n)# Combine all model metrics\n\nall_models_metrics |> arrange(.metric, .estimate)# Arrange the results to compare performance. Sort them to identify the best models --> rmse the smaller the better; mae the smaller the better; rsq the bigger the better.\n```\n:::\n\n\nBased on the results above we can conclude that:\n\n1.  The linear regression model with recipe 2 has the best MAE. However, that model is troublesome (remember the warning) and it would be very hard to interpret.\n\n2.  The decision tree model with recipe 4 is the best model metrics wise, it has the second lowest MAE and the best RMSE and RSQ. Decision tree are usually easy to interpret but using all the predictors might be challenging and unnecessary (more parsimonious models that lead to similar results are preferred).\n\n3.  Unfortunately, the more parsimonious/simpler models (e.g., recipe 1 and 3 models and lasso with recipe 2) are not close to the best models in terms of performance. So, it seems that none of the above models is truly optimal for our needs. However, we have some indications of the direction for our next model: include more than 2/3 predictors (not performing well enough)  but not all predictors (too complex and harder to interpret).  We will use the activities below to try to improve the current model performance.\n\n#### Activity 3: Comparing and evaluating models new models - 30 minutes\n\n*\\[Write code just below each instruction; finally use MS Teams R - Forum channel for help on the in class activities/homework or if you have other questions\\]*\n\n::: {.panel-tabset .nav-pills}\n## Activity 3a.\n\nBuild a new recipe that use only numeric variables with strong positive correlation with sales price (log version) [hint: remember to create correlation matrices]. Make sure all the independent variables are standardized. Include also `overall_cond` and `neighborhood` as dummy variables. Call this recipe as \"recipe_3a\".\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\n\n```\n:::\n\n\n## Activity 3b.\n\nDefine a new ridge regression model using parsnip. Name the model as \"linear_mod_ridge\". Make sure to use the right arguments to perform ridge regression.\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\n\n```\n:::\n\n\n## Activity 3c.\n\nCreate 5 new workflows: 1) linear regression model with recipe_3a (recipe3a_workflow_reg); 2) lasso regression model with recipe_3a (recipe3a_workflow_lasso); 3) ridge regression model with recipe1 (recipe1_workflow_ridge); 4) ridge regression model with recipe2 (recipe2_workflow_ridge); 5) ridge regression model with recipe_3a. (recipe3_workflow_ridge).\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\n\n```\n:::\n\n\n## Activity 3d.\n\nMake predictions using the 5 new workflows. Call the prediction as recipe3a_predictions_reg, recipe3a_predictions_lasso, recipe1_predictions_ridge, recipe2_predictions_ridge, recipe3a_predictions_ridge.\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\n\n```\n:::\n\n\n## Activity 3e.\n\nCalculate for each one of the above RMSE, MAE and R-squared. Use similar naming convention used in the example above. Make sure to put the metrics in a new tibble. Make sure the tibble also contains the previous models results. Compare the results with the old ones. Which one is the best model in making predictions?\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\n\n```\n:::\n\n:::\n\n\n::: {.cell autorun='false' min-lines='3'}\n```{webr}\n#| autorun: false\n#| min-lines: 3\n\n\n#Code Practice Chunk\n\n```\n:::\n\n\n\n![**On completing another R coding class!**](gif/congrats.webp){fig-align=\"left\" width=\"700\"}\n\n::: {style=\"height: 30vh;\"}\n:::\n\n![**Happy Halloween!**](images/halloween2.png){fig-align=\"left\" width=\"700\"}\n\n\n\n<script src=\"https://cdn.jsdelivr.net/npm/canvas-confetti@1.5.1/dist/confetti.browser.min.js\"></script>\n<script>\n  var confettiTriggered = false;\n  var duration = 15 * 1000;\n  var animationEnd = Date.now() + duration;\n  var defaults = { startVelocity: 30, spread: 360, ticks: 60, zIndex: 0 };\n \n  function randomInRange(min, max) {\n    return Math.random() * (max - min) + min;\n  }\n \n  // Function to start the confetti effect\n  function startConfetti() {\n    if (!confettiTriggered) {\n      confettiTriggered = true; // Prevent confetti from starting multiple times\n      var interval = setInterval(function() {\n        var timeLeft = animationEnd - Date.now();\n \n        if (timeLeft <= 0) {\n          clearInterval(interval); // Stop the animation when the duration ends\n        }\n \n        var particleCount = 50 * (timeLeft / duration);\n        // Since particles fall down, start a bit higher than random\n        confetti({ \n          ...defaults, \n          particleCount, \n          origin: { x: randomInRange(0.1, 0.3), y: Math.random() - 0.2 } \n        });\n        confetti({ \n          ...defaults, \n          particleCount, \n          origin: { x: randomInRange(0.7, 0.9), y: Math.random() - 0.2 } \n        });\n      }, 250);\n    }\n  }\n \n  // Event listener to detect when the user scrolls to the bottom of the page\n  window.addEventListener(\"scroll\", function() {\n    // Check if the user has scrolled to the bottom of the page\n    if ((window.innerHeight + window.scrollY) >= document.body.offsetHeight) {\n      startConfetti();\n    }\n  });\n</script>\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/countdown-0.4.0/countdown.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/countdown-0.4.0/countdown.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}