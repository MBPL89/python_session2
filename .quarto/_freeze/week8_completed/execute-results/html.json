{
  "hash": "32163e9e04ec0a63b4f7150a0f35dba0",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Week 8 Completed File\"\nauthor: \"Biagio Palese\"\nengine: knitr\nformat: live-html\nwebr:\n  packages:\n    - tidyverse\n    - tidymodels\neditor_options: \n  chunk_output_type: console\n---\n\n```{=html}\n<style>\n/* Target the first tab (Demo) */\n.nav-pills .nav-item:nth-child(1) .nav-link {\n  background-color: #FFC20A; /* Yellow */\n  color: white;\n}\n\n/* Target the second tab (Example) */\n.nav-pills .nav-item:nth-child(2) .nav-link {\n  background-color: #5D3A9B; /* Purple */\n  color: white;\n}\n\n/* Target the third tab (Exercise) */\n.nav-pills .nav-item:nth-child(3) .nav-link {\n  background-color: #0C7BDC; /* Blue */\n  color: white;\n}\n\n/* Target the fourth tab (Activity) */\n.nav-pills .nav-item:nth-child(4) .nav-link {\n  background-color: #E66100; /* Orange */\n  color: white;\n}\n\n/* Target the fifth tab (Activity2) */\n.nav-pills .nav-item:nth-child(5) .nav-link {\n  background-color: #40B0A6; /* teal */\n  color: white;\n}\n\n/* Active tab styling */\n.nav-pills .nav-link.active {\n  background-color: #2E7D32; /* Customize active tab color */\n  color: white;\n}\n</style>\n```\n::: {.cell}\n\n:::\n\n\n\n# Introduction to Basic Modeling\n\n![Coding is fun!!!](gif/coding_is_fun.gif){width=\"700\"}\n\nIn the second part of the course we will leverage the following resources:\n\n-   The Tidy Modeling with R book [second portion of the course book](https://www.tmwr.org)\n-   The Tidy Models website [second portion of the course website](https://www.tidymodels.org)\n\n#### Link to other resources\n\n-   Internal help: [posit support](https://support.posit.co/hc/en-us){style=\"text-decoration: underline; color: rgb(0, 0, 255) !important;\"}\n\n-   External help: [stackoverflow](https://stackoverflow.com/search?q=rstudio&s=25d8522e-3191-4bf2-ae3b-ccad762aeca9){style=\"text-decoration: underline; color: rgb(0, 0, 255) !important;\"}\n\n-   Additional materials: [posit resources](https://posit.co/resources/){style=\"text-decoration: underline; color: rgb(0, 0, 255) !important;\"}\n\nWhile I use the book as a reference the materials provided to you are custom made and include more activities and resources.\n\nIf you understand the materials covered in this document there is no need to refer to other resources.\n\nIf you have any troubles with the materials don't hesitate to contact me or check the above resources.\n\n![Data Science model: Artwork by @allison_horst](images/environmental-data-science-r4ds-general.png)\n\n::: {#custom-timer}\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"countdown\" id=\"timer_8d6ac76a\" data-warn-when=\"180\" data-update-every=\"1\" data-play-sound=\"true\" data-blink-colon=\"true\" tabindex=\"0\" style=\"right:0;bottom:0;font-size:1.5rem;\">\n<div class=\"countdown-controls\"><button class=\"countdown-bump-down\">&minus;</button><button class=\"countdown-bump-up\">&plus;</button></div>\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">10</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n\n:::\n:::\n\n:::\n\n\n```{=html}\n<style>\n  /* Fix the custom timer div to the top-right corner */\n  #custom-timer {\n    position: fixed;    /* Ensure the timer stays fixed when scrolling */\n    top: 70px;          /* Distance from the top */\n    right: 10px;        /* Distance from the right */\n    font-size: 1rem;   /* Small font size */\n    z-index: 9999;       /* Ensure it stays on top of other elements */\n    width: 250px;        /* Set a small width for the timer */\n    text-align: center;  /* Center the text */\n  }\n</style>\n```\n\n### Class Objectives\n\n-   Introduce the `tidymodels` framework, highlighting its comprehensive ecosystem designed to streamline the predictive modeling process in R.\n\n-   Establish a foundational modeling using `tidymodels`, emphasizing practical application through the exploration of the `ames` dataset.\n\n## Load packages\n\nThis is a critical task:\n\n-   Every time you open a new R session you will need to load the packages.\n\n-   Failing to do so will incur in the most common errors among beginners (e.g., \" could not find function 'x' \" or \"object 'y' not found\").\n\n-   So please always remember to load your packages by running the `library` function for each package you will use in that specific session ü§ù\n\n\n::: {.cell autorun='true' startover='false'}\n```{webr}\n#| autorun: true\n#| startover: false\nlibrary(tidyverse)\n#install.packages(\"tidymodels\") \nlibrary(tidymodels)\n\n```\n:::\n\n\n## The tidymodels Ecosystem in action\n\n![Tidymodels in Action](images/tidymodels_ecosystem_inpractice.png)\n\nAfter the theoretical introduction let's check the tidymodels ecosystem in action with an overview of the , highlighting its components like recipes, parsnip, workflows, etc., and how it integrates with the tidyverse. But first we need to:\n\n### Exploring the `ames` Dataset\n\nThe `ames` housing dataset will be used in this portion of the course. Let's explore the dataset to understand the variables and the modeling objective (predicting house prices).\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\n\ndata(ames, package = \"modeldata\")\n?ames \nglimpse(ames) \nstr(ames)\n```\n:::\n\n\n#### Activity 0: Get to know and Explore the ames dataset.\n\nThe objective here is to leverage what we learned so far to understand the data. Write all the code you think is needed to complete the task in the chunks below - 10 minutes\n\n::: callout-warning\nKeep in mind that `Sale_Price` will be our dependent variable in supervised modeling. Identify possible relevant independent variables by producing charts and descriptive stats. Think also about potential manipulations needed on those variables/creating new variables.\n:::\n\n::: {.panel-tabset .nav-pills}\n## Data manipulations\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\n# There is not a specific set of manipulations or manipulations needed that I will put here. However, I would start by getting to know my dependent variable first. For example, full descriptive stats table of the dependent variable. Then do the same with geom_histogram. At this point I would move to some potential independent variable that I believe should have an impact to my dependent variable. Also in this case descriptive stats table, group_by a categorical one also would help using the dependend variable as variable for the stats. On the data viz size for sure some distributions, ranking, evolutions but especially correlation charts between your dependent and numerical independent variables should be created (aka geom_point & geom_smooth)\n\n```\n:::\n\n\n## Data visualizations\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\n# There is not a specific set of manipulations or manipulations needed that I will put here. However, I would start by getting to know my dependent variable first. For example, full descriptive stats table of the dependent variable. Then do the same with geom_histogram. At this point I would move to some potential independent variable that I believe should have an impact to my dependent variable. Also in this case descriptive stats table, group_by a categorical one also would help using the dependend variable as variable for the stats. On the data viz size for sure some distributions, ranking, evolutions but especially correlation charts between your dependent and numerical independent variables should be created (aka geom_point & geom_smooth)\n\n```\n:::\n\n\n## More manipulations\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\n# There is not a specific set of manipulations or manipulations needed that I will put here. However, I would start by getting to know my dependent variable first. For example, full descriptive stats table of the dependent variable. Then do the same with geom_histogram. At this point I would move to some potential independent variable that I believe should have an impact to my dependent variable. Also in this case descriptive stats table, group_by a categorical one also would help using the dependend variable as variable for the stats. On the data viz size for sure some distributions, ranking, evolutions but especially correlation charts between your dependent and numerical independent variables should be created (aka geom_point & geom_smooth)\n\n```\n:::\n\n\n## More visualizations\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\n# There is not a specific set of manipulations or manipulations needed that I will put here. However, I would start by getting to know my dependent variable first. For example, full descriptive stats table of the dependent variable. Then do the same with geom_histogram. At this point I would move to some potential independent variable that I believe should have an impact to my dependent variable. Also in this case descriptive stats table, group_by a categorical one also would help using the dependend variable as variable for the stats. On the data viz size for sure some distributions, ranking, evolutions but especially correlation charts between your dependent and numerical independent variables should be created (aka geom_point & geom_smooth)\n\n```\n:::\n\n:::\n\nThe ames housing dataset will be used as a case study for this section of the course, demonstrating the practical application of predictive modeling techniques.\n\n## Part 2: Data Preprocessing with recipes\n\n![Recipes: Artwork by \\@allison_horst](images/recipes.png){width=\"700\"}\n\nLet's delve into the concept of a recipe in the context of data preprocessing within the tidymodels framework, followed by creating a simple recipe for the ames dataset.\n\n### Understanding the Concept of a `recipe` in Data Preprocessing\n\nIn the tidymodels ecosystem, a recipe is a blueprint that outlines how to prepare your data for modeling. It's a series of instructions or steps ( [full list available here](https://recipes.tidymodels.org/reference/index.html){style=\"text-decoration: underline; color: rgb(0, 0, 255) !important;\"}) that transform raw data into a format more suitable for analysis, ensuring that the preprocessing is systematic and reproducible. Here‚Äôs why recipes are pivotal in the data science workflow:\n\n-   ***Standardization and Normalization***: Recipes can include steps to standardize (scale) or normalize (transform) numerical data, ensuring that variables are on comparable scales. Main functions:\n\n    -   `step_log()`: Applies log transformation\n    -   `step_center()`: Normalizes numeric columns by forcing mean = 0.\n    -   `step_scale()`: Normalizes numeric columns by forcing sd = 1\n    -   `step_normalize()`: Normalizes numeric columns by forcing mean = 0 & sd = 1.\n\n-   ***Handling Missing Data***: They allow you to specify methods for imputing missing values, ensuring the model uses a complete dataset for training. Main functions:\n\n    -   `step_impute_median()`: Imputes missing numeric values with the median\n\n    -   `step_impute_mean()`: Imputes missing numeric values with the average\n\n    -   `step_impute_mode()`: Imputes missing categorical values with the mode\n\n-   ***Encoding Categorical Variables:*** Recipes describe how to convert categorical variables into a format that models can understand, typically through one-hot encoding or other encoding strategies. Main functions:\n\n    -   `step_dummy()`: Creates dummy variables for categorical predictors\n    -   `step_other()`: Collapses infrequent factors into an \"other\" category\n\n-   ***Feature Engineering:*** Recipe can include steps for creating new features from existing ones, enhancing the model's ability to learn from the data. Main functions:\n\n    -   `step_mutate()`: Creates a new feature from an existing one\n    -   `step_interact()`: Creates interaction terms between features\n\n-   ***Data Filtering and Columns Selection:*** They can also be used to select specific variables or filter rows based on certain criteria, tailoring the dataset to the modeling task. Main functions:\n\n    -   `step_select()`: Selects specific variables to retain in the dataset\n    -   `step_slice()`: Randomly selects a subset of rows\n\n::: callout-important\nBy defining these steps in a recipe, you create a reproducible set of transformations that can be applied to any dataset of the same structure and across multiple models.\n:::\n\nThis reproducibility is crucial for maintaining the integrity of your modeling process, especially when moving from a development environment to production.\n\n### Example: Creating a `recipe` for the `ames` dataset\n\nLet's create a recipe for the ames housing dataset, focusing on preprocessing steps that are commonly required for this type of data.\n\n::: callout-note\nOur goal is to predict house sale prices, so we'll include steps to log-transform the target variable (to address skewness) and normalize the numerical predictors.\n:::\n\n![Normalize Distribution and More: Artwork by \\@allison_horst](images/not_normal.png){width=\"700\"}\n\n::: {.panel-tabset .nav-pills}\n## Step 1\n\nDefine the recipe\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\n# Define the recipe\names_recipe <- recipe(Sale_Price ~ ., data = ames) \n\n# Review the recipe\names_recipe\n\n```\n:::\n\n\n## Step 2\n\nLog-transform the `Sale_Price` to normalize its distribution\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\n# Define the recipe\names_recipe <- recipe(Sale_Price ~ ., data = ames) |>\n  # Log-transform the Sale_Price to normalize its distribution\n  step_log(Sale_Price, base = 10) \n\n\n# Review the recipe steps\names_recipe\n\n```\n:::\n\n\n## Step 3\n\nNormalize all numerical predictors\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\n# Define the recipe\names_recipe <- recipe(Sale_Price ~ ., data = ames) |>\n  # Log-transform the Sale_Price to normalize its distribution\n  step_log(Sale_Price, base = 10) |>\n  # Normalize numerical predictors\n  step_normalize(all_numeric_predictors()) \n\n# Review the recipe steps\names_recipe\n\n```\n:::\n\n\n## Step 4\n\nPrepare for modeling by converting factor/categorical variables into dummy variables\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\n# Define the recipe\names_recipe <- recipe(Sale_Price ~ ., data = ames) |>\n  # Log-transform the Sale_Price to normalize its distribution\n  step_log(Sale_Price, base = 10) |>\n  # Normalize numerical predictors\n  step_normalize(all_numeric_predictors()) |>\n  # Prepare for modeling by converting factor variables into dummy variables\n  step_dummy(all_nominal_predictors())\n\n# Review the recipe steps\names_recipe\n\n```\n:::\n\n\n## Putting the steps together\n\n\n::: {.cell}\n```{webr}\npreprocessed_ames <- recipe(Sale_Price ~ ., data = ames) |>\n  # Log-transform the Sale_Price to normalize its distribution\n  step_log(Sale_Price, base = 10) |>\n  # Normalize numerical predictors\n  step_normalize(all_numeric_predictors()) |>\n  # Prepare for modeling by converting factor variables into dummy variables\n  step_dummy(all_nominal_predictors()) |> \n  prep() |> \n  bake(new_data = NULL) \n\names |> \n  select(Sale_Price, Lot_Area, Overall_Cond)\n\n#VS preprocessed_ames:\n\npreprocessed_ames |> \n  select(Sale_Price, Lot_Area, contains(\"Overall_Cond\")) |> \n  print(width = Inf)\n```\n:::\n\n\nDo you notice the differences between `ames` and `preprocessed_ames`?\n:::\n\n::: callout-warning\n-   I showed you just a few columns affected by the recipe preprocessing steps. However, due to the `step_dummy(all_nominal_predictors())` step, all the factor/categorical variables are converted from factor into dummies variables.\n\n-   Check the difference in the number of columns available in `ames` and `preprocessed_ames` to see the impact of that preprocessing step. You will see that such preprocessing step resulted in many more columns so always question if you need all the existing columns in your analysis or if you need to transform all of them into dummy variables.\n:::\n\n#### Recap of the `recipe` steps\n\nThe ames_recipe outlines a series of preprocessing steps tailored to the ames dataset:\n\n-   The `step_log()` function applies a log transformation to the `Sale_Price`, which is a common technique to normalize the distribution of the target variable in regression tasks.\n\n-   The `step_normalize()` function scales numerical predictors, ensuring they contribute equally to the model's predictions.\n\n-   The `step_dummy()` function converts categorical variables into a series of binary (0/1) columns, enabling models to incorporate this information.\n\nBy preparing the data with this recipe, we enhance the dataset's suitability for predictive modeling, improving the potential accuracy and interpretability of the resulting models.\n\n::: callout-important\nIt is critical that you verify that the preprocessing steps applied lead to the desired outcomes. For example, did the log transformation addressed the normality issue of your target variable?\n:::\n\nRecipes in tidymodels provide a flexible and powerful way to specify and execute a series of data preprocessing steps, ensuring that your data science workflow is both efficient and reproducible.\n\n#### Activity 1: Preprocessing with `recipes`. Write the code to complete the below tasks - 10 minutes\n\n*\\[Write code just below each instruction; finally use MS Teams R - Forum channel for help on the in class activities/homework or if you have other questions\\]*\n\n::: {.panel-tabset .nav-pills}\n## Activity 1a.\n\nStandardize mean and sd of the `Gr_Liv_Area`, and impute missing values for `Lot_Frontage` with median, and for `Garage_Type` with mean.\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\n\n# Start your recipe with predicting Sale_Price from all other variables\names_recipe1a <- recipe(Sale_Price ~ ., data = ames)|>\n  step_normalize(Gr_Liv_Area) |> \n  step_impute_median(Lot_Frontage) |> \n  step_impute_mean(Garage_Type)\n  \n\n```\n:::\n\n\nWhat do you notice?\n\n## Activity 1b.\n\nAdd to activity 1a the steps required to create dummy variables for the `Neighborhood` column, and to normalize the `Year_Built` column to force mean = 0.\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\n\names_recipe1b <- recipe(Sale_Price ~ ., data = ames)|>\n  step_normalize(Gr_Liv_Area) |> \n  step_impute_median(Lot_Frontage) |> \n  step_impute_mean(Garage_Type) |>\n  step_dummy(Neighborhood) |> \n  step_normalize(Year_Built)\n\n```\n:::\n\n\n## Activity 1c.\n\nAdd to activity 1b the steps required to create an interaction term between the `Overall_Qual` and the `Gr_Liv_Area` columns and to standardize mean and sd of the `Lot_Area` variable.\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\n\names_recipe1c <- recipe(Sale_Price ~ ., data = ames)|>\n  step_normalize(Gr_Liv_Area) |> \n  step_impute_median(Lot_Frontage) |> \n  step_impute_mean(Garage_Type) |>\n  step_dummy(Neighborhood) |> \n  step_normalize(Year_Built) |> \n  step_interact(Overall_Qual ~ Gr_Liv_Area) |> \n  step_normalize(Lot_Area)\n  \n\n  \n  \n\n    \n```\n:::\n\n\n## Activity 1d.\n\nAdd to activity 1c the steps required to keep only the `Sale_Price`, `Overall_Qual`, `Gr_Liv_Area` and `Year_Built` columns. Moreover, slice the dataset to randomly select only 500 rows.\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\names_recipe1d <- recipe(Sale_Price ~ ., data = ames)|>\n  step_normalize(Gr_Liv_Area) |> \n  step_impute_median(Lot_Frontage) |> \n  step_impute_mean(Garage_Type) |> \n  step_dummy(Neighborhood) |> \n  step_normalize(Year_Built) |> \n  step_interact(Overall_Qual ~ Gr_Liv_Area) |> \n  step_normalize(Lot_Area) |> \n  step_select(Sale_Price,Overall_Qual, Gr_Liv_Area, Year_Built ) |> \n  step_slice(1:500)\n\n# Remember to use prep and bake to check outcome of your preprocessing\n?prep()\n?bake()\n\n```\n:::\n\n:::\n\n\n::: {.cell autorun='false' min-lines='3'}\n```{webr}\n#| autorun: false\n#| min-lines: 3\n\n\n#Code Practice Chunk\n\n```\n:::\n\n\n## Part 3: Building Models with `parsnip` in tidymodels\n\nThe `parsnip` package is a cornerstone of the tidymodels ecosystem designed to streamline and unify the process of model specification across various types of models and machine learning algorithms.\n\nUnlike traditional approaches that require navigating the syntax and idiosyncrasies of different modeling functions and packages, parsnip abstracts this complexity into a consistent and intuitive interface. Here's why parsnip stands out:\n\n### Unified Interface:\n\nparsnip offers a single, cohesive syntax for specifying a wide range of models ( [full list available here](https://parsnip.tidymodels.org/reference/index.html){style=\"text-decoration: underline; color: rgb(0, 0, 255) !important;\"}), from simple linear regression to complex ensemble methods and deep learning. This uniformity simplifies learning and reduces the cognitive load when switching between models or trying new methodologies. Main models:\n\n## Regression Models:\n\n::: {.panel-tabset .nav-pills}\n## Linear Regression\n\n[When to Use]{.underline}: Ideal for predicting continuous outcomes when the relationship between independent variables and the dependent variable is linear.\n\n[Example]{.underline}: Predicting house prices based on attributes like size (square footage), number of bedrooms, and age of the house.\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\nlinear_reg() |> \n  set_engine(\"lm\") |> \n  set_mode(\"regression\")\n```\n:::\n\n\n## Ridge Regression\n\n[When to Use]{.underline}: Useful when dealing with multicollinearity (independent variables are highly correlated) in your data or when you want to prevent overfitting by adding a penalty (L2 regularization) to the size of coefficients.\n\n[Example]{.underline}: Predicting employee salaries using a range of correlated features like years of experience, level of education, and job role.\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\nlinear_reg(penalty = 0.1, mixture = 0) |> \n  set_engine(\"glmnet\") |> \n  set_mode(\"regression\")\n```\n:::\n\n\n## Lasso Regression\n\n[When to Use]{.underline}: Similar to Ridge regression but can shrink some coefficients to zero, effectively performing variable selection (L1 regularization). Useful for models with a large number of predictors, where you want to identify a simpler model with fewer predictors.\n\n[Example]{.underline}: Selecting the most impactful factors affecting the energy efficiency of buildings from a large set of potential features.\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\nlinear_reg(penalty = 0.1, mixture = 1) |> \n  set_engine(\"glmnet\") |> \n  set_mode(\"regression\")\n```\n:::\n\n:::\n\n## Classification & Tree-Based Models\n\n::: callout-important\nTree-based models can be used also for regression. We will see later how to change their purpose based on the analysis objective.\n:::\n\n::: {.panel-tabset .nav-pills}\n## Logistic Regression:\n\n[When to Use]{.underline}: Suited for binary classification problems where you predict the probability of an outcome that can be one of two possible states.\n\n[Example]{.underline}: Determining whether an email is spam or not based on features like the frequency of certain words, sender reputation, and email structure.\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\nlogistic_reg() |> \n  set_engine(\"glm\") |> \n  set_mode(\"classification\")\n```\n:::\n\n\n## Decision Tree:\n\n[When to Use]{.underline}: Good for classification and regression with a dataset that includes non-linear relationships. Decision trees are interpretable and can handle both numerical and categorical data.\n\n[Example]{.underline}: Predicting customer churn based on a variety of customer attributes such as usage patterns, service complaints, and demographic information.\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\ndecision_tree() |> \n  set_engine(\"rpart\") |> \n  set_mode(\"classification\") # or \"regression\"\n```\n:::\n\n\n## Gradient Boosted Models:\n\n[When to Use]{.underline}: When you require a robust predictive model for both regression and classification that can automatically handle missing values and does not require scaling of data. It builds trees in a sequential manner, minimizing errors from previous trees.\n\n[Example]{.underline}: Predicting with high accuracy complex outcomes such as future sales amounts across different stores and products.\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\nboost_tree() |> \n  set_engine(\"xgboost\") |> \n  set_mode(\"classification\") # or \"regression\"\n```\n:::\n\n\n## Random Forest:\n\n[When to Use]{.underline}: Suitable for scenarios where high accuracy is crucial, and the model can handle being a \"black box.\" Works well for both classification and regression tasks and is robust against overfitting.\n\n[Example]{.underline}: Diagnosing diseases from patient records where there's a need to consider a vast number of symptoms and test results.\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\nrand_forest(mtry = 2, trees = 1000) |> \n  set_engine(\"ranger\") |> \n  set_mode(\"classification\") # or \"regression\"\n```\n:::\n\n:::\n\n## Deep Learning: Neural Network\n\n[When to Use]{.underline}: Best for capturing complex, nonlinear relationships in high-dimensional data. Neural networks excel in tasks like image recognition, natural language processing, and time series prediction.\n\n[Example]{.underline}: Recognizing handwritten digits where the model needs to learn from thousands of examples of handwritten digits.\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\nmlp() |> \n  set_engine(\"keras\") |> \n  set_mode(\"regression\") # or \"classification\"\n```\n:::\n\n\n### What we learned from the above?\n\n-   Each of these model functions can be specified with parsnip's unified interface, allowing you to easily switch between them or try new models with minimal syntax changes. The choice of model and the specific parameters/arguments (penalty, mtry, trees, etc.) should be guided by the nature of your data, the problem at hand, and possibly iterative model tuning processes like training-test sampling and cross-validation.\n\n-   Engine Independence: Behind the scenes, many models can be fitted using different computational engines (e.g., lm, glmnet, ranger). parsnip allows you to specify the model once and then choose the computational engine separately, providing flexibility and making it easy to compare the performance of different implementations.\n\n-   Integration with tidymodels: parsnip models seamlessly integrate with other tidymodels packages, such as `recipes` for data preprocessing, `workflows` for bundling preprocessing and modeling steps, and `tune` for hyperparameter optimization.\n\n![Parsnip: Artwork by \\@allison_horst](images/parsnip.png)\n\n### Starting Simple: Specifying a Linear Regression Model Using `parsnip`\n\nLinear regression is one of the most fundamental statistical and machine learning methods, used for predicting a continuous outcome based on one or more predictors. Let's specify a linear regression model using parsnip for the ames housing dataset, aiming to predict the sale price of houses from their characteristics.\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\n# Specify a linear regression model\nlinear_mod <- linear_reg() |>\n  set_engine(\"lm\") |>\n  set_mode(\"regression\")\n\n# Review the model specification\nlinear_mod\n```\n:::\n\n\nIn this specification:\n\n-   `linear_reg()` initiates the specification of a linear regression model. At this stage, we're defining the type of model but not yet fitting it to data.\n\n-   `set_engine(\"lm\")` selects the computational engine to use for fitting the model. Here, we use \"lm\", which stands for linear models, a base R function well-suited for fitting these types of models.\n\n-   `set_mode(\"regression\")` indicates that we are performing a regression task, predicting a continuous outcome.\n\n::: callout-important\n-   This process outlines the model we intend to use without binding it to specific data. The next steps typically involve integrating the prepared data (using a recipe), fitting the model to training data, and evaluating its performance.\n\nparsnip's design makes these steps straightforward and consistent across different types of models, enhancing the reproducibility and scalability of your modeling work.\n:::\n\nThrough the abstraction provided by parsnip, model specification becomes not only more straightforward but also more adaptable to different contexts and needs, facilitating a smoother workflow in predictive modeling projects.\n\n#### Activity 2: Exploring modeling with `parsnip`. - 10 minutes:\n\n*\\[Write code just below each instruction; finally use MS Teams R - Forum channel for help on the in class activities/homework or if you have other questions\\]*\n\n::: {.panel-tabset .nav-pills}\n## Activity 2a.\n\nFit a linear regression model (lm) predicting `Sale_Price` using `Lot_Area`\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\n\names_recipe_a <- recipe(Sale_Price ~ Lot_Area, data = ames)\nlinear_mod2 <- linear_reg() |> \n  set_engine(\"lm\") |> \n  set_mode(\"regression\")\n\n```\n:::\n\n\nWhat do you notice?\n\n## Activity 2b.\n\nFit a linear regression model (lm) predicting `Sale_Price` using `Gr_Liv_Area` and `Lot_Area`\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\names_recipe_b <- recipe(Sale_Price ~ Lot_Area+Gr_Liv_Area, data = ames)\nlinear_mod2 <- linear_reg() |> \n  set_engine(\"lm\") |> \n  set_mode(\"regression\")\n\n```\n:::\n\n\nWhat do you notice? How different it is from the model in Activity 2a?\n\n## Activity 2c.\n\nFit a decision tree model to predict `Sale_Price` using `Lot_Area` and `Year_Built`.\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\names_recipe_c <- recipe(Sale_Price ~ Lot_Area+Year_Built, data = ames)\ndecision_tree1 <- decision_tree() |> \n  set_engine(\"rpart\") |> \n  set_mode(\"classification\")\n\n```\n:::\n\n\nWhat do you notice?\n\n## Activity 2d.\n\nFit a decision tree mode model to predict `Sale_Price` using `Gr_Liv_Area` and `Lot_Area`.\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\names_recipe_d <- recipe(Sale_Price ~ Lot_Area+Gr_Liv_Area, data = ames)\ndecision_tree1 <- decision_tree() |> \n  set_engine(\"rpart\") |> \n  set_mode(\"classification\")\n\n```\n:::\n\n\nWhat do you notice? How does it differ to the model in Activity 2c?\n:::\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\n\n\n#Code Practice Chunk\n\n```\n:::\n\n\n::: callout-caution\nAs you probably noticed the specification of the models in 2a and 2b and the one of the models in 2c and 2d is identical.\n:::\n\n::: callout-important\nWhy and how is that possible? The secret is that parsnip doesn't require specifications of variables or formulas to be used in/by the models. That piece of information is acquired when we use `workflows` and we actually `fit` the models. Because of this, it is extremely easy to reuse defined models across different datasets and analyses.\n:::\n\n## Part 4: Integrating Preprocessing and Modeling with `workflows`\n\nThe `workflows` package is a powerful component of the `tidymodels` ecosystem designed to streamline the modeling process. It provides a cohesive framework that binds together preprocessing steps (`recipes`) and model specifications (`parsnip`) into a single, unified object. This integration enhances the modeling workflow by ensuring consistency, reproducibility, and efficiency.\n\n#### Key Advantages of Using `workflows`:\n\n-   **Unified Process:** By encapsulating both preprocessing (e.g., feature engineering, normalization) and modeling within a single object, `workflows` simplifies the execution of the entire modeling pipeline. This unified approach reduces the risk of mismatches or errors between the data preprocessing and modeling stages.\n\n-   **Reproducibility:** `workflows` makes your analysis more reproducible by explicitly linking preprocessing steps to the model. This linkage ensures that anyone reviewing your work can see the complete path from raw data to model outputs.\n\n-   **Flexibility and Efficiency:** It allows for easy experimentation with different combinations of preprocessing steps and models. Since preprocessing and model specification are encapsulated together, switching out components to test different hypotheses or improve performance becomes more streamlined.\n\n#### Building and Fitting a Model Using `workflows`\n\nTo demonstrate the practical application of `workflows`, let's consider the `ames` housing dataset, where our goal is to predict house sale prices based on various features. We'll use the linear regression model specified with `parsnip` and the preprocessing recipe developed with `recipes`.\n\n::: callout-warning\nMake sure that `ames_recipe`, a preprocessing steps object created with `recipes`, is in your environment (check it to be true by running `ls()`) and that also `linear_mod`, a linear regression model specified with `parsnip`, is available in your environment\n:::\n\n::: {.panel-tabset .nav-pills}\n## Create a workflow:\n\nCreate the workflow by combining the recipe and model\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\names_workflow <- workflow() |>\n  add_recipe(ames_recipe) |>\n  add_model(linear_mod)\n\n```\n:::\n\n\n## Fit a workflow\n\nFit the workflow to the Ames housing data\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\names_fit <- fit(ames_workflow, data = ames)\n```\n:::\n\n\n## Create and fit\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\names_fit2 <- workflow() |>\n  add_recipe(ames_recipe) |>\n  add_model(linear_mod) |> \n  fit(data = ames)\n\n```\n:::\n\n\n## Review results\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\names_fit\names_fit |> tidy() #check values\names_fit |> tidy() |> print(n=Inf)# check all values\n```\n:::\n\n:::\n\nIn this process:\n\n-   We start by creating a new `workflow` object, to which we add our previously defined preprocessing recipe (`ames_recipe`) and linear regression model (`linear_mod`).\n\n-   The `add_recipe()` and `add_model()` functions are used to incorporate the preprocessing steps and model specification into the workflow, respectively.\n\n-   The `fit()` function is then used to apply this workflow to the `ames` dataset, executing the preprocessing steps on the data before fitting the specified model.\n\n-   The result is a fitted model object that includes both the preprocessing transformations and the model's learned parameters, ready for evaluation or prediction on new data.\n\nThis example underscores how `workflows` elegantly combines data preprocessing and model fitting into a cohesive process, streamlining the journey from raw data to actionable insights.\n\nBy leveraging `workflows`, data scientists can maintain a clear, organized, and efficient modeling pipeline. `workflows` epitomizes the philosophy of `tidymodels` in promoting clean, understandable, and reproducible modeling practices.\n\nThrough its structured approach to integrating preprocessing and modeling, `workflows` facilitates a seamless transition across different stages of the predictive modeling process.\n\n::: callout-note\nWe will ignore the results interpretation for now because we don't know how to evaluate the models yet but you can see how simple it is to create and run a modeling workflow.\n:::\n\n#### Activity 3: Streamline Modeling with Workflow. - 15 minutes\n\n*\\[Write code just below each instruction; finally use MS Teams R - Forum channel for help on the in class activities/homework or if you have other questions\\]*\n\n::: {.panel-tabset .nav-pills}\n## Activity 3a.\n\nDefine a recipe (\"recipe_3a\") for the following preprocessing steps: standardize `Gr_Liv_Area` and encode `Neighborhood` as dummy variables. Specify a linear regression model using the 'lm' engine (lm_reg_3a). Create a workflow, add the `recipe_3a` recipe and the `lm_reg_3a` model, then fit the workflow to the `ames` data.\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\n\nrecipe_3a <- recipe(Sale_Price ~., data= ames) |> \n  step_normalize(Gr_Liv_Area) |> \n  step_dummy(Neighborhood)#or\n\nrecipe_3a2 <- recipe(Sale_Price ~ Gr_Liv_Area + Neighborhood, data= ames) |> \n  step_normalize(Gr_Liv_Area) |> \n  step_dummy(Neighborhood)\n\nlm_reg_3a <- linear_reg() |> \n  set_engine(\"lm\") |> \n  set_mode(\"regression\")\n\nworkflow_3a <- workflow() |> \n  add_recipe(recipe_3a) |> \n  add_model(lm_reg_3a) |> \n  fit( data = ames)\n\n\nworkflow_3a |> tidy()\n\n```\n:::\n\n\n## Activity 3b.\n\nDefine a recipe (\"recipe_3b\") for the following preprocessing steps: compute a new \"Total_flr_SF\" variable equal to `First_Flr_SF + Second_Flr_SF` and encode `MS_SubClass` to make sure that infrequent values fall inside the `other` category. Specify a classification decision tree model (\"tree_3b\"). Create a workflow, add the `recipe_3b` recipe and the `tree_3b` model, then fit the workflow to the `ames` data.\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\nrecipe_3b <- recipe(Sale_Price ~., data= ames) |> \n  step_mutate(Total_flr_SF= First_Flr_SF + Second_Flr_SF) |> \n  step_other(MS_SubClass)\n\ntree_3b <- decision_tree() |> \n  set_engine(\"rpart\") |> \n  set_mode(\"classification\")\n\nworkflow_3b <- workflow() |> \n  add_recipe(recipe_3b) |> \n  add_model(tree_3b) |> \n  fit( data = ames)\n\n\nworkflow_3b |> tidy()\n\n```\n:::\n\n\n## Activity 3c.\n\nDefine a recipe (\"recipe_3c\") for the following preprocessing steps: scale 'Lot_Area' and sample only 500 rows. Specify a linear regression model using 'glmnet' engine (\"glmnet_reg_3c\") using `penalty = 0.1` and `mixture = 0`. Create a workflow, add the `recipe_3c` recipe and the `glmnet_reg_3c` model, then fit the workflow to the `ames` data.\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\n\nrecipe_3c <- recipe(Sale_Price ~., data= ames) |> \n  step_scale(Lot_Area) |> \n  step_slice(1:500)\n\nglmnet_reg_3c <- linear_reg(penalty = 0.1, mixture = 0) |> \n  set_engine(\"glmnet\") |> \n  set_mode(\"regression\")\n\nworkflow_3c <- workflow() |> \n  add_recipe(recipe_3c) |> \n  add_model(glmnet_reg_3c) |> \n  fit( data = ames)\n\n\nworkflow_3c |> tidy()\n```\n:::\n\n\n## Activity 3d.\n\nDefine a recipe (\"recipe_3d\") for the following preprocessing steps: log transform `Lot_Frontage` and encode all nominal variables as dummy variables. Specify a decision tree model (\"tree_mod_3d\"). Create a workflow, add the `recipe_3d` recipe and the `tree_mod_3d` model, then fit the workflow to the `ames` data.\n\n\n::: {.cell autorun='false' completion='true' min-lines='3'}\n```{webr}\n#| autorun: false\n#| completion: true\n#| min-lines: 3\nrecipe_3d <- recipe(Sale_Price ~., data= ames) |> \n  step_log(Lot_Frontage) |> \n  step_dummy(all_nominal_predictors())\n\ntree_mod_3d <- decision_tree() |> \n  set_engine(\"rpart\") |> \n  set_mode(\"regression\")\n\nworkflow_3d <- workflow() |> \n  add_recipe(recipe_3d) |> \n  add_model(tree_mod_3d) |> \n  fit( data = ames)\n\n\nworkflow_3d |> tidy()\n\n```\n:::\n\n:::\n\n\n::: {.cell autorun='false' min-lines='3'}\n```{webr}\n#| autorun: false\n#| min-lines: 3\n\n\n#Code Practice Chunk\n\n```\n:::\n\n\n::: callout-important\nThis is enough for our first coding modeling class with tidymodels. More to come in the next weeks but please make sure you understand everything we have covered so far! Please reach out for help and ask clarifications if needed. We are here to help ;-)\n:::\n\n![**On completing another R coding class!**](gif/congrats.webp){fig-align=\"left\" width=\"700\"}\n\n\n\n<script src=\"https://cdn.jsdelivr.net/npm/canvas-confetti@1.5.1/dist/confetti.browser.min.js\"></script>\n<script>\n  var confettiTriggered = false;\n  var duration = 15 * 1000;\n  var animationEnd = Date.now() + duration;\n  var defaults = { startVelocity: 30, spread: 360, ticks: 60, zIndex: 0 };\n \n  function randomInRange(min, max) {\n    return Math.random() * (max - min) + min;\n  }\n \n  // Function to start the confetti effect\n  function startConfetti() {\n    if (!confettiTriggered) {\n      confettiTriggered = true; // Prevent confetti from starting multiple times\n      var interval = setInterval(function() {\n        var timeLeft = animationEnd - Date.now();\n \n        if (timeLeft <= 0) {\n          clearInterval(interval); // Stop the animation when the duration ends\n        }\n \n        var particleCount = 50 * (timeLeft / duration);\n        // Since particles fall down, start a bit higher than random\n        confetti({ \n          ...defaults, \n          particleCount, \n          origin: { x: randomInRange(0.1, 0.3), y: Math.random() - 0.2 } \n        });\n        confetti({ \n          ...defaults, \n          particleCount, \n          origin: { x: randomInRange(0.7, 0.9), y: Math.random() - 0.2 } \n        });\n      }, 250);\n    }\n  }\n \n  // Event listener to detect when the user scrolls to the bottom of the page\n  window.addEventListener(\"scroll\", function() {\n    // Check if the user has scrolled to the bottom of the page\n    if ((window.innerHeight + window.scrollY) >= document.body.offsetHeight) {\n      startConfetti();\n    }\n  });\n</script>\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/countdown-0.4.0/countdown.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/countdown-0.4.0/countdown.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}